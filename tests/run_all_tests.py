#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
PersonalCookingRecipe - 3ãƒãƒ£ãƒ³ãƒãƒ«çµ±åˆãƒ¬ã‚·ãƒ”ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚
"""

import os
import sys
import subprocess
import argparse
import logging
import json
import time
import tempfile
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "config"))

# ãƒ†ã‚¹ãƒˆçµæœæ ¼ç´ã‚¯ãƒ©ã‚¹
class TestResults:
    def __init__(self):
        self.results = {}
        self.start_time = None
        self.end_time = None
        self.total_duration = 0
        self.summary = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'skipped_tests': 0,
            'error_tests': 0,
            'success_rate': 0.0
        }
    
    def start_timing(self):
        """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–‹å§‹æ™‚é–“è¨˜éŒ²"""
        self.start_time = time.time()
    
    def end_timing(self):
        """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµ‚äº†æ™‚é–“è¨˜éŒ²"""
        self.end_time = time.time()
        self.total_duration = self.end_time - self.start_time
    
    def add_test_result(self, test_name: str, result: Dict[str, Any]):
        """ãƒ†ã‚¹ãƒˆçµæœè¿½åŠ """
        self.results[test_name] = result
        
        # ã‚µãƒãƒªãƒ¼æ›´æ–°
        self.summary['total_tests'] += result.get('total', 0)
        self.summary['passed_tests'] += result.get('passed', 0)
        self.summary['failed_tests'] += result.get('failed', 0)
        self.summary['skipped_tests'] += result.get('skipped', 0)
        self.summary['error_tests'] += result.get('errors', 0)
    
    def calculate_success_rate(self):
        """æˆåŠŸç‡è¨ˆç®—"""
        if self.summary['total_tests'] > 0:
            self.summary['success_rate'] = (
                self.summary['passed_tests'] / self.summary['total_tests']
            )
        else:
            self.summary['success_rate'] = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§çµæœã‚’è¿”ã™"""
        return {
            'test_results': self.results,
            'summary': self.summary,
            'timing': {
                'start_time': self.start_time,
                'end_time': self.end_time,
                'total_duration': self.total_duration
            }
        }


class TestRunner:
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_root: Path = PROJECT_ROOT):
        self.project_root = project_root
        self.tests_dir = project_root / "tests"
        self.results = TestResults()
        self.logger = self._setup_logger()
        
        # ãƒ†ã‚¹ãƒˆè¨­å®š
        self.test_modules = {
            'api_connections': {
                'file': 'test_api_connections.py',
                'description': 'APIæ¥ç¶šãƒ†ã‚¹ãƒˆ',
                'markers': [],
                'required': True
            },
            'system_integration': {
                'file': 'test_system_integration.py',
                'description': 'ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ',
                'markers': ['integration'],
                'required': True
            },
            'security': {
                'file': 'test_security.py',
                'description': 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ',
                'markers': ['security'],
                'required': True
            },
            'performance': {
                'file': 'test_performance.py',
                'description': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ',
                'markers': ['slow'],
                'required': False
            },
            'ui_components': {
                'file': 'test_ui_components.py',
                'description': 'UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ†ã‚¹ãƒˆ',
                'markers': ['ui'],
                'required': False
            },
            'e2e_selenium': {
                'file': 'test_e2e_selenium.py',
                'description': 'E2E Seleniumãƒ†ã‚¹ãƒˆ',
                'markers': ['e2e'],
                'required': False
            },
            'macos_integration': {
                'file': 'test_macos_integration.py',
                'description': 'macOSçµ±åˆãƒ†ã‚¹ãƒˆ',
                'markers': ['macos'],
                'required': sys.platform == 'darwin'
            }
        }
    
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚°è¨­å®š"""
        logger = logging.getLogger('test_runner')
        logger.setLevel(logging.INFO)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        console_handler = logging.StreamHandler(sys.stdout)
        console_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(console_formatter)
        logger.addHandler(console_handler)
        
        return logger
    
    def run_single_test(self, test_name: str, test_config: Dict[str, Any]) -> Dict[str, Any]:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        test_file = self.tests_dir / test_config['file']
        
        if not test_file.exists():
            return {
                'status': 'skipped',
                'reason': 'Test file not found',
                'total': 0,
                'passed': 0,
                'failed': 0,
                'skipped': 1,
                'errors': 0,
                'duration': 0
            }
        
        self.logger.info(f"ğŸ§ª å®Ÿè¡Œä¸­: {test_config['description']}")
        
        # pytestå¼•æ•°æ§‹ç¯‰
        pytest_args = [
            sys.executable, '-m', 'pytest',
            str(test_file),
            '-v',
            '--tb=short',
            '--strict-markers',
            '--json-report',
            '--json-report-file=/tmp/test_report.json'
        ]
        
        # ãƒãƒ¼ã‚«ãƒ¼æŒ‡å®š
        if test_config['markers']:
            marker_expr = ' or '.join(test_config['markers'])
            pytest_args.extend(['-m', marker_expr])
        
        start_time = time.time()
        
        try:
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            result = subprocess.run(
                pytest_args,
                capture_output=True,
                text=True,
                timeout=300,  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                cwd=self.project_root
            )
            
            duration = time.time() - start_time
            
            # JSON ãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿
            try:
                with open('/tmp/test_report.json', 'r') as f:
                    report_data = json.load(f)
                
                summary = report_data.get('summary', {})
                
                test_result = {
                    'status': 'completed',
                    'return_code': result.returncode,
                    'total': summary.get('total', 0),
                    'passed': summary.get('passed', 0),
                    'failed': summary.get('failed', 0),
                    'skipped': summary.get('skipped', 0),
                    'errors': summary.get('error', 0),
                    'duration': duration,
                    'stdout': result.stdout[-1000:],  # æœ€å¾Œã®1000æ–‡å­—
                    'stderr': result.stderr[-1000:] if result.stderr else ''
                }
                
                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
                if result.returncode == 0:
                    self.logger.info(f"âœ… æˆåŠŸ: {test_config['description']} ({duration:.1f}ç§’)")
                else:
                    self.logger.error(f"âŒ å¤±æ•—: {test_config['description']} (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {result.returncode})")
                
            except (FileNotFoundError, json.JSONDecodeError):
                # JSONãƒ¬ãƒãƒ¼ãƒˆãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆ
                test_result = {
                    'status': 'completed',
                    'return_code': result.returncode,
                    'total': 1,
                    'passed': 1 if result.returncode == 0 else 0,
                    'failed': 0 if result.returncode == 0 else 1,
                    'skipped': 0,
                    'errors': 0,
                    'duration': duration,
                    'stdout': result.stdout[-1000:],
                    'stderr': result.stderr[-1000:] if result.stderr else ''
                }
        
        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            test_result = {
                'status': 'timeout',
                'return_code': -1,
                'total': 1,
                'passed': 0,
                'failed': 0,
                'skipped': 0,
                'errors': 1,
                'duration': duration,
                'stdout': '',
                'stderr': 'Test execution timed out'
            }
            self.logger.error(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {test_config['description']}")
        
        except Exception as e:
            duration = time.time() - start_time
            test_result = {
                'status': 'error',
                'return_code': -2,
                'total': 1,
                'passed': 0,
                'failed': 0,
                'skipped': 0,
                'errors': 1,
                'duration': duration,
                'stdout': '',
                'stderr': str(e)
            }
            self.logger.error(f"ğŸ’¥ ä¾‹å¤–: {test_config['description']} - {e}")
        
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            try:
                os.unlink('/tmp/test_report.json')
            except FileNotFoundError:
                pass
        
        return test_result
    
    def run_all_tests(self, include_optional: bool = False, specific_tests: Optional[List[str]] = None) -> TestResults:
        """å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.results = TestResults()
        self.results.start_timing()
        
        self.logger.info("ğŸš€ Recipe Monitor åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")
        self.logger.info(f"ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ: {self.project_root}")
        self.logger.info(f"ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.tests_dir}")
        
        # å®Ÿè¡Œã™ã‚‹ãƒ†ã‚¹ãƒˆã‚’æ±ºå®š
        tests_to_run = {}
        
        for test_name, test_config in self.test_modules.items():
            should_run = False
            
            # ç‰¹å®šãƒ†ã‚¹ãƒˆæŒ‡å®šãŒã‚ã‚‹å ´åˆ
            if specific_tests:
                should_run = test_name in specific_tests
            else:
                # å¿…é ˆãƒ†ã‚¹ãƒˆã¾ãŸã¯ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãƒ†ã‚¹ãƒˆå«ã‚ã‚‹å ´åˆ
                should_run = test_config['required'] or include_optional
            
            if should_run:
                tests_to_run[test_name] = test_config
        
        self.logger.info(f"ğŸ“‹ å®Ÿè¡Œäºˆå®šãƒ†ã‚¹ãƒˆæ•°: {len(tests_to_run)}")
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        for i, (test_name, test_config) in enumerate(tests_to_run.items(), 1):
            self.logger.info(f"ğŸ“Š é€²æ—: {i}/{len(tests_to_run)} - {test_name}")
            
            result = self.run_single_test(test_name, test_config)
            self.results.add_test_result(test_name, result)
        
        self.results.end_timing()
        self.results.calculate_success_rate()
        
        self.logger.info("ğŸ å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå®Œäº†")
        
        return self.results
    
    def generate_report(self, results: TestResults, output_file: Optional[Path] = None) -> str:
        """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_lines = [
            "=" * 80,
            "ğŸ§ª PersonalCookRecipe - åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆ",
            "=" * 80,
            "",
            f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {results.total_duration:.2f}ç§’",
            f"ğŸ’» ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {sys.platform}",
            f"ğŸ Python: {sys.version.split()[0]}",
            "",
            "ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼",
            "-" * 40,
            f"  ç·ãƒ†ã‚¹ãƒˆæ•°: {results.summary['total_tests']}",
            f"  âœ… æˆåŠŸ: {results.summary['passed_tests']}",
            f"  âŒ å¤±æ•—: {results.summary['failed_tests']}",
            f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {results.summary['skipped_tests']}",
            f"  ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {results.summary['error_tests']}",
            f"  ğŸ“ˆ æˆåŠŸç‡: {results.summary['success_rate']:.1%}",
            "",
            "ğŸ” è©³ç´°çµæœ",
            "-" * 40
        ]
        
        for test_name, result in results.results.items():
            test_config = self.test_modules.get(test_name, {})
            description = test_config.get('description', test_name)
            
            status_icon = {
                'completed': 'âœ…' if result['return_code'] == 0 else 'âŒ',
                'skipped': 'â­ï¸',
                'timeout': 'â°',
                'error': 'ğŸ’¥'
            }.get(result['status'], 'â“')
            
            report_lines.extend([
                f"{status_icon} {description}",
                f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result['status']}",
                f"   å®Ÿè¡Œæ™‚é–“: {result['duration']:.2f}ç§’",
                f"   æˆåŠŸ/å¤±æ•—/ã‚¹ã‚­ãƒƒãƒ—/ã‚¨ãƒ©ãƒ¼: {result['passed']}/{result['failed']}/{result['skipped']}/{result['errors']}",
            ])
            
            if result['stderr']:
                report_lines.append(f"   ã‚¨ãƒ©ãƒ¼: {result['stderr'][:100]}...")
            
            report_lines.append("")
        
        # æ¨å¥¨äº‹é …
        report_lines.extend([
            "ğŸ’¡ æ¨å¥¨äº‹é …",
            "-" * 40
        ])
        
        if results.summary['failed_tests'] > 0:
            report_lines.append("â€¢ å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„")
        
        if results.summary['skipped_tests'] > 0:
            report_lines.append("â€¢ ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã®å‰ææ¡ä»¶ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
        if results.summary['success_rate'] < 0.8:
            report_lines.append("â€¢ æˆåŠŸç‡ãŒ80%ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šæ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
        if results.summary['success_rate'] >= 0.9:
            report_lines.append("â€¢ ğŸ‰ å„ªç§€ãªçµæœã§ã™ï¼ã‚·ã‚¹ãƒ†ãƒ ã¯å®‰å®šã—ã¦å‹•ä½œã—ã¦ã„ã¾ã™")
        
        report_lines.extend([
            "",
            "=" * 80,
            "ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆçµ‚äº†",
            "=" * 80
        ])
        
        report_content = "\n".join(report_lines)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        if output_file:
            output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            self.logger.info(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›: {output_file}")
        
        return report_content
    
    def save_json_results(self, results: TestResults, output_file: Path):
        """JSONå½¢å¼ã§ã®çµæœä¿å­˜"""
        json_data = {
            'metadata': {
                'project': 'PersonalCookingRecipe',
                'timestamp': datetime.now().isoformat(),
                'platform': sys.platform,
                'python_version': sys.version.split()[0]
            },
            'results': results.to_dict()
        }
        
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ’¾ JSONçµæœä¿å­˜: {output_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='PersonalCookRecipe - åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ãƒ†ã‚¹ãƒˆç¨®åˆ¥:
  api_connections     - APIæ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆå¿…é ˆï¼‰
  system_integration - ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆå¿…é ˆï¼‰
  security           - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆï¼ˆå¿…é ˆï¼‰
  performance        - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
  ui_components      - UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
  e2e_selenium       - E2E Seleniumãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
  macos_integration  - macOSçµ±åˆãƒ†ã‚¹ãƒˆï¼ˆmacOSã§ã¯å¿…é ˆï¼‰

ä¾‹:
  python run_all_tests.py                      # å¿…é ˆãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
  python run_all_tests.py --include-optional   # å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
  python run_all_tests.py --tests api_connections security  # ç‰¹å®šãƒ†ã‚¹ãƒˆã®ã¿
  python run_all_tests.py --output-dir ./test_results      # çµæœå‡ºåŠ›å…ˆæŒ‡å®š
        """
    )
    
    parser.add_argument(
        '--include-optional',
        action='store_true',
        help='ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãƒ†ã‚¹ãƒˆã‚‚å«ã‚ã¦å®Ÿè¡Œ'
    )
    
    parser.add_argument(
        '--tests',
        nargs='*',
        help='å®Ÿè¡Œã™ã‚‹ç‰¹å®šã®ãƒ†ã‚¹ãƒˆåã‚’æŒ‡å®š'
    )
    
    parser.add_argument(
        '--output-dir',
        type=Path,
        default=PROJECT_ROOT / 'test_results',
        help='ãƒ†ã‚¹ãƒˆçµæœã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./test_resultsï¼‰'
    )
    
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='æœ€å°é™ã®å‡ºåŠ›ã®ã¿'
    )
    
    parser.add_argument(
        '--json-only',
        action='store_true',
        help='JSONçµæœã®ã¿ä¿å­˜ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰'
    )
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
    if args.quiet:
        logging.getLogger('test_runner').setLevel(logging.WARNING)
    
    # ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼åˆæœŸåŒ–
    runner = TestRunner()
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    try:
        results = runner.run_all_tests(
            include_optional=args.include_optional,
            specific_tests=args.tests
        )
        
        # çµæœå‡ºåŠ›
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSONçµæœä¿å­˜
        json_file = args.output_dir / f"test_results_{timestamp}.json"
        runner.save_json_results(results, json_file)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if not args.json_only:
            report_file = args.output_dir / f"test_report_{timestamp}.txt"
            report_content = runner.generate_report(results, report_file)
            
            if not args.quiet:
                print("\n" + report_content)
        
        # çµ‚äº†ã‚³ãƒ¼ãƒ‰æ±ºå®š
        if results.summary['failed_tests'] > 0 or results.summary['error_tests'] > 0:
            print(f"\nâŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {results.summary['failed_tests']} failures, {results.summary['error_tests']} errors")
            sys.exit(1)
        elif results.summary['total_tests'] == 0:
            print("\nâš ï¸ å®Ÿè¡Œã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            sys.exit(2)
        else:
            print(f"\nâœ… å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸ: {results.summary['passed_tests']}/{results.summary['total_tests']} passed")
            sys.exit(0)
    
    except KeyboardInterrupt:
        print("\nğŸ›‘ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šãƒ†ã‚¹ãƒˆå®Ÿè¡ŒãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(130)
    
    except Exception as e:
        print(f"\nğŸ’¥ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()