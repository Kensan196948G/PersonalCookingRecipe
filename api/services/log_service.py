#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ­ã‚°ã‚µãƒ¼ãƒ“ã‚¹ - ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ãƒ»è§£æãƒ»é…ä¿¡

Author: Recipe-DevUI Agent
Date: 2025-08-08
"""

import json
import asyncio
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime, timezone, timedelta
import logging
import re
from collections import defaultdict

from models.api_models import LogEntry, LogLevel, LogComponent, LogFilters

logger = logging.getLogger(__name__)

class LogService:
    """ãƒ­ã‚°ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, log_dir: Optional[Path] = None):
        """åˆæœŸåŒ–"""
        self.log_dir = log_dir or Path("../logs")
        self.log_dir.mkdir(exist_ok=True)
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.app_log_file = self.log_dir / "application.log"
        self.error_log_file = self.log_dir / "error.log"
        self.api_log_file = self.log_dir / "api.log"
        
        # ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ­ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆæœ€æ–°1000ä»¶ï¼‰
        self._log_cache: List[LogEntry] = []
        self._max_cache_size = 1000
        
        # ãƒ­ã‚°ãƒ‘ãƒ¼ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
        self._log_pattern = re.compile(
            r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) - '
            r'(?P<component>\w+) - '
            r'(?P<level>\w+) - '
            r'(?P<message>.*)'
        )
        
        logger.info(f"ğŸ“‹ LogService initialized with log_dir: {self.log_dir}")
    
    async def get_logs(self, level: Optional[str] = None, component: Optional[str] = None, limit: int = 100) -> List[LogEntry]:
        """ãƒ­ã‚°å–å¾—ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä»˜ãï¼‰"""
        try:
            # æœ€æ–°ãƒ­ã‚°ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«èª­ã¿è¾¼ã¿
            await self._refresh_log_cache()
            
            filtered_logs = self._log_cache.copy()
            
            # ãƒ¬ãƒ™ãƒ«ãƒ•ã‚£ãƒ«ã‚¿
            if level:
                level_enum = LogLevel(level.upper())
                filtered_logs = [log for log in filtered_logs if log.level == level_enum]
            
            # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ•ã‚£ãƒ«ã‚¿
            if component:
                component_enum = LogComponent(component.lower())
                filtered_logs = [log for log in filtered_logs if log.component == component_enum]
            
            # æœ€æ–°é †ã§ã‚½ãƒ¼ãƒˆã—ã¦åˆ¶é™
            filtered_logs.sort(key=lambda l: l.timestamp, reverse=True)
            result = filtered_logs[:limit]
            
            logger.debug(f"ğŸ“œ Retrieved {len(result)} logs (filtered from {len(filtered_logs)})")
            return result
            
        except Exception as e:
            logger.error(f"Error getting logs: {e}")
            return []
    
    async def get_logs_since(self, since: datetime, limit: int = 100) -> List[LogEntry]:
        """æŒ‡å®šæ—¥æ™‚ä»¥é™ã®ãƒ­ã‚°ã‚’å–å¾—"""
        try:
            await self._refresh_log_cache()
            
            # æŒ‡å®šæ—¥æ™‚ä»¥é™ã®ãƒ­ã‚°ã‚’ãƒ•ã‚£ãƒ«ã‚¿
            since_logs = [
                log for log in self._log_cache 
                if log.timestamp >= since
            ]
            
            # æœ€æ–°é †ã§ã‚½ãƒ¼ãƒˆã—ã¦åˆ¶é™
            since_logs.sort(key=lambda l: l.timestamp, reverse=True)
            result = since_logs[:limit]
            
            logger.debug(f"ğŸ“… Retrieved {len(result)} logs since {since}")
            return result
            
        except Exception as e:
            logger.error(f"Error getting logs since {since}: {e}")
            return []
    
    async def _refresh_log_cache(self):
        """ãƒ­ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°"""
        try:
            new_logs = []
            
            # å„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
            log_files = [
                (self.app_log_file, LogComponent.SYSTEM),
                (self.error_log_file, LogComponent.SYSTEM),
                (self.api_log_file, LogComponent.API)
            ]
            
            for log_file, default_component in log_files:
                if log_file.exists():
                    file_logs = await self._parse_log_file(log_file, default_component)
                    new_logs.extend(file_logs)
            
            # æ™‚é–“é †ã§ã‚½ãƒ¼ãƒˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            new_logs.sort(key=lambda l: l.timestamp, reverse=True)
            self._log_cache = new_logs[:self._max_cache_size]
            
        except Exception as e:
            logger.error(f"Error refreshing log cache: {e}")
    
    async def _parse_log_file(self, log_file: Path, default_component: LogComponent) -> List[LogEntry]:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦LogEntryãƒªã‚¹ãƒˆã‚’ä½œæˆ"""
        try:
            logs = []
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆå¤§ãã™ãã‚‹å ´åˆã¯æœ«å°¾ã®ã¿èª­ã‚€ï¼‰
            file_size = log_file.stat().st_size
            max_size = 10 * 1024 * 1024  # 10MB
            
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                if file_size > max_size:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã„å ´åˆã¯æœ«å°¾ã‹ã‚‰èª­ã‚€
                    f.seek(max(0, file_size - max_size))
                    # æœ€åˆã®ä¸å®Œå…¨ãªè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                    f.readline()
                
                lines = f.readlines()
                
                # æœ€æ–°500è¡Œã«åˆ¶é™
                if len(lines) > 500:
                    lines = lines[-500:]
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                log_entry = self._parse_log_line(line, default_component)
                if log_entry:
                    logs.append(log_entry)
            
            return logs
            
        except Exception as e:
            logger.error(f"Error parsing log file {log_file}: {e}")
            return []
    
    def _parse_log_line(self, line: str, default_component: LogComponent) -> Optional[LogEntry]:
        """ãƒ­ã‚°è¡Œã‚’è§£æã—ã¦LogEntryã‚’ä½œæˆ"""
        try:
            # æ¨™æº–ãƒ­ã‚°å½¢å¼ã‚’è©¦è¡Œ
            match = self._log_pattern.match(line)
            
            if match:
                timestamp_str = match.group('timestamp')
                component_str = match.group('component')
                level_str = match.group('level')
                message = match.group('message')
                
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å¤‰æ›
                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')
                timestamp = timestamp.replace(tzinfo=timezone.utc)
                
                # ãƒ¬ãƒ™ãƒ«å¤‰æ›
                try:
                    level = LogLevel(level_str.upper())
                except ValueError:
                    level = LogLevel.INFO
                
                # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå¤‰æ›
                try:
                    component = LogComponent(component_str.lower())
                except ValueError:
                    component = default_component
                
                return LogEntry(
                    timestamp=timestamp,
                    level=level,
                    component=component,
                    message=message
                )
            else:
                # æ¨™æº–å½¢å¼ã§ãªã„å ´åˆã¯ç°¡æ˜“ãƒ‘ãƒ¼ã‚¹
                return LogEntry(
                    timestamp=datetime.now(timezone.utc),
                    level=LogLevel.INFO,
                    component=default_component,
                    message=line
                )
                
        except Exception as e:
            logger.error(f"Error parsing log line: {e}")
            return None
    
    async def add_log_entry(self, level: LogLevel, component: LogComponent, message: str, 
                           metadata: Optional[Dict[str, Any]] = None):
        """ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ã®ç›´æ¥è¿½åŠ ç”¨ï¼‰"""
        try:
            log_entry = LogEntry(
                timestamp=datetime.now(timezone.utc),
                level=level,
                component=component,
                message=message,
                metadata=metadata
            )
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ 
            self._log_cache.insert(0, log_entry)
            if len(self._log_cache) > self._max_cache_size:
                self._log_cache = self._log_cache[:self._max_cache_size]
            
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ï¼ˆéåŒæœŸï¼‰
            await self._write_log_to_file(log_entry)
            
        except Exception as e:
            logger.error(f"Error adding log entry: {e}")
    
    async def _write_log_to_file(self, log_entry: LogEntry):
        """ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿"""
        try:
            # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
            if log_entry.level in [LogLevel.ERROR, LogLevel.CRITICAL]:
                target_file = self.error_log_file
            elif log_entry.component == LogComponent.API:
                target_file = self.api_log_file
            else:
                target_file = self.app_log_file
            
            # ãƒ­ã‚°è¡Œã‚’ä½œæˆ
            log_line = (
                f"{log_entry.timestamp.strftime('%Y-%m-%d %H:%M:%S,%f')[:-3]} - "
                f"{log_entry.component.value} - "
                f"{log_entry.level.value} - "
                f"{log_entry.message}"
            )
            
            # éåŒæœŸã§ãƒ•ã‚¡ã‚¤ãƒ«è¿½è¨˜
            with open(target_file, 'a', encoding='utf-8') as f:
                f.write(log_line + '\n')
                
        except Exception as e:
            logger.error(f"Error writing log to file: {e}")
    
    async def get_log_statistics(self) -> Dict[str, Any]:
        """ãƒ­ã‚°çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            await self._refresh_log_cache()
            
            # ãƒ¬ãƒ™ãƒ«åˆ¥é›†è¨ˆ
            level_counts = defaultdict(int)
            component_counts = defaultdict(int)
            
            # æ™‚é–“åˆ¥é›†è¨ˆï¼ˆéå»24æ™‚é–“ï¼‰
            now = datetime.now(timezone.utc)
            hourly_counts = defaultdict(int)
            
            for log in self._log_cache:
                level_counts[log.level.value] += 1
                component_counts[log.component.value] += 1
                
                # 24æ™‚é–“ä»¥å†…ã®ãƒ­ã‚°ã‚’æ™‚é–“åˆ¥ã«é›†è¨ˆ
                if now - log.timestamp <= timedelta(hours=24):
                    hour_key = log.timestamp.strftime('%Y-%m-%d %H:00')
                    hourly_counts[hour_key] += 1
            
            # ã‚¨ãƒ©ãƒ¼ç‡è¨ˆç®—
            total_logs = len(self._log_cache)
            error_logs = level_counts.get('ERROR', 0) + level_counts.get('CRITICAL', 0)
            error_rate = (error_logs / total_logs * 100) if total_logs > 0 else 0
            
            return {
                'total_logs': total_logs,
                'level_distribution': dict(level_counts),
                'component_distribution': dict(component_counts),
                'hourly_distribution': dict(hourly_counts),
                'error_rate': round(error_rate, 2),
                'last_updated': datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error getting log statistics: {e}")
            return {}
    
    async def search_logs(self, query: str, limit: int = 50) -> List[LogEntry]:
        """ãƒ­ã‚°å†…å®¹æ¤œç´¢"""
        try:
            await self._refresh_log_cache()
            
            query_lower = query.lower()
            matching_logs = [
                log for log in self._log_cache
                if query_lower in log.message.lower()
            ]
            
            # æœ€æ–°é †ã§ã‚½ãƒ¼ãƒˆã—ã¦åˆ¶é™
            matching_logs.sort(key=lambda l: l.timestamp, reverse=True)
            result = matching_logs[:limit]
            
            logger.debug(f"ğŸ” Found {len(result)} logs matching query: {query}")
            return result
            
        except Exception as e:
            logger.error(f"Error searching logs: {e}")
            return []
    
    async def get_error_summary(self, hours: int = 24) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        try:
            await self._refresh_log_cache()
            
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)
            
            error_logs = [
                log for log in self._log_cache
                if log.level in [LogLevel.ERROR, LogLevel.CRITICAL] and 
                   log.timestamp >= cutoff_time
            ]
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            error_patterns = defaultdict(int)
            component_errors = defaultdict(int)
            
            for log in error_logs:
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æœ€åˆã®50æ–‡å­—ã§ãƒ‘ã‚¿ãƒ¼ãƒ³åŒ–
                pattern = log.message[:50] + "..." if len(log.message) > 50 else log.message
                error_patterns[pattern] += 1
                component_errors[log.component.value] += 1
            
            return {
                'total_errors': len(error_logs),
                'error_patterns': dict(error_patterns),
                'component_errors': dict(component_errors),
                'time_range_hours': hours,
                'most_recent_error': error_logs[0].model_dump() if error_logs else None
            }
            
        except Exception as e:
            logger.error(f"Error getting error summary: {e}")
            return {}